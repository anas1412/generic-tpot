export const personas = [
  {
    name: "ml anon",
    subtitle: "learning deep ml, mathematics & neural nets",
    articleTitle: "musings on the elegance of machine learning",
    articleSubtitle: "a journey into esoteric mathematical realms",
    articles: [
      {
        title: "a journey into esoteric mathematical realms",
        body: `ah, machine learning—the darling of contemporary computational
        innovation, and yet, so often misunderstood, even by its most ardent
        practitioners. today, i wish to indulge in a discourse not on the
        banalities of hyperparameter tuning or the mechanical labor of data
        preprocessing, but on the transcendent elegance of the field itself,
        viewed through the lens of mathematical sophistication and philosophical
        inquiry.`,
      },
      {
        title: "the alchemy of the p-value",
        body: `let us begin, as one must, with the ubiquitous yet woefully
        misinterpreted p-value. often treated as the divine oracle of
        statistical inference, it is, in truth, a mere probability—conditional
        and thus inherently limited. but in the hands of the machine learning
        aficionado, the p-value becomes more than a metric; it transforms into
        an epistemic cornerstone. the p-value is not just a number; it is a
        measure of our epistemic humility, a way of quantifying the tenuous
        thread by which our hypotheses dangle over the abyss of falsifiability.
        yet, i often muse: why do we cling so fervently to the p-value when its
        esoteric sibling, bayesian inference, offers us a far more nuanced lens?
        bayesian priors—those ethereal encapsulations of our pre-existing
        beliefs—serve as the poetic counterpoint to the cold determinism of
        frequentist inference.`,
      },
      {
        title: "the hessian's symphony",
        body: `let us now turn our attention to the derivative matrix, a spectral
        symphony of elegance and power. the hessian, a second-order derivative
        matrix, speaks to the soul of optimization. when the hessian's
        eigenvalues sing in unison, descending the gradient becomes a dance of
        precision and grace.`,
      },
      {
        title: "the euclidean geometry of neural nets",
        body: `let us now turn our attention to the derivative matrix, a spectral
        symphony of elegance and power. the hessian, a second-order derivative
        matrix, speaks to the soul of optimization. when the hessian's
        eigenvalues sing in unison, descending the gradient becomes a dance of
        precision and grace.`,
      },
      {
        title: "the euclidean geometry of neural nets",
        body: `let us now turn our attention to the derivative matrix, a spectral
        symphony of elegance and power. the hessian, a second-order derivative
        matrix, speaks to the soul of optimization. when the hessian's
        eigenvalues sing in unison, descending the gradient becomes a dance of
        precision and grace.`,
      },
      {
        title: "on the geometry of learning",
        body: `machine learning, at its core, is an exercise in navigating
        high-dimensional manifolds. consider the gradient descent algorithm: a
        balletic dance of partial derivatives through a riemannian landscape.
        each update step is a moment of profound mathematical beauty, an
        iteration of hope against the backdrop of loss landscapes riddled with
        local minima and saddle points. the esoterica of hessians and
        eigenvalues—ah, here lies the heart of the pretension machine learning
        demands! the curvature of the loss function, encoded in the second
        derivative matrix, speaks to the soul of optimization. when the hessian
        's eigenvalues sing in unison, descending the gradient becomes a
        mere triviality; when they do not, one must summon the spectral wisdom
        of quasi-newton methods or stochastic approximations.`,
      },
      {
        title: "neural networks: the esoteric choir of nonlinearities",
        body: `is there anything more sublime than a neural network? these
        multi-layered behemoths of matrix multiplication and activation
        functions are the modern age's answer to divine intervention. relu,
        sigmoid, softmax—each nonlinearity is a hymn in the esoteric choir of
        function approximation. consider, if you will, the universal
        approximation theorem. this elegant result—a subtle interplay of
        topology and linear algebra—proclaims that a sufficiently wide neural
        network can approximate any continuous function. but, dear reader, do
        not be deceived by its simplicity. for what is a function, truly? is it
        merely a mapping, or does it represent a deeper metaphysical truth about
        the nature of the universe?`,
      },
      {
        title: "the pretension of esotericism",
        body: `i could write volumes on the spectral clustering of eigenfaces, the
        fourier transform as a lens into convolutional layers, or the exotic
        calculus of backpropagation. yet, to delve too deeply would be to risk
        alienating the uninitiated, and i, for one, have no desire to gatekeep
        the sublime. instead, i leave you with this: machine learning is not
        merely a toolkit for prediction. it is a philosophy, a meditation on the
        nature of knowledge, and a celebration of the esoteric. to understand it
        is to embrace not only the mathematics but the profound humility that
        comes with knowing how little we truly understand about the world. in
        the end, perhaps the most important lesson is this: the beauty of
        machine learning lies not in its ability to predict, but in its ability
        to ask questions.`,
      },
    ],
  },
  {
    name: "an ardent philosopher",
    subtitle: "merging existential musings with elegant code",
    articleTitle: "the metaphysics of code",
    articleSubtitle: "programming as a reflection of human consciousness",
    articles: [
      {
        title: "the metaphysics of code",
        body: `to write code is to engage in a dialogue with the infinite. each
        function, a question posed to the cosmos. each return statement, a tentative
        answer. the act of programming is not merely technical—it is deeply
        existential, a meditation on the nature of problem-solving and the boundaries
        of what we can model.`,
      },
      {
        title: "debugging as a journey of self-discovery",
        body: `to debug is to confront one's own fallibility. every error message
        is a mirror, reflecting our assumptions and blind spots. but in this
        confrontation lies growth. debugging teaches us humility, patience, and
        the art of thinking through the system.`,
      },
      {
        title: "the ontology of a null pointer",
        body: `a null pointer is not just a bug; it is a philosophical statement.
        it tells us of the void, of the absence of meaning where we expected
        connection. to handle null gracefully is to embrace the uncertainty
        inherent in life.`,
      },
    ],
  },
  {
    name: "quantum hermit",
    subtitle: "dwelling on quantum mysteries and probabilistic realities",
    articleTitle: "entangled musings",
    articleSubtitle: "probabilities, superposition, and everything in between",
    articles: [
      {
        title: "entangled musings",
        body: `the quantum realm is a whisper, an echo of possibilities. to understand
        superposition is to accept that reality is not fixed, but a dance of
        probabilities. each quantum state is a narrative, waiting to collapse
        into certainty.`,
      },
      {
        title: "the poetry of schrodinger's equation",
        body: `schrodinger's equation is not merely a mathematical construct; it is
        a hymn to the wave-like nature of existence. it describes how the
        probability amplitude evolves—a symphony of differential equations that
        echoes across the universe.`,
      },
    ],
  },
  {
    name: "data whisperer",
    subtitle: "finding beauty in patterns hidden in noise",

    articleTitle: "whispers of the data",
    articleSubtitle: "extracting secrets from the chaos",
    articles: [
      {
        title: "the elegance of clustering",
        body: `clustering is not just a statistical exercise; it is an art. to group
        data points is to uncover latent structures, hidden stories that the
        dataset yearns to tell. whether through k-means or hierarchical methods,
        clustering whispers to us of order within chaos.`,
      },
      {
        title: "the curse and blessing of dimensionality",
        body: `high-dimensional data is both a labyrinth and a treasure chest. in
        the vast space of features, patterns emerge, but only for those patient
        enough to listen. dimensionality reduction is the act of distilling
        complexity into essence.`,
      },
    ],
  },
  {
    name: "an engineer",
    subtitle: "navigating the moral dilemmas of modern tech",

    articleTitle: "ethics in the age of algorithms",
    articleSubtitle: "questions we should be asking",
    articles: [
      {
        title: "the cost of convenience",
        body: `every app, every algorithm has a cost—not just in computation but in
        privacy, agency, and ethics. as technologists, we must ask: at what point
        does convenience become exploitation?`,
      },
      {
        title: "bias in the machine",
        body: `algorithms are not neutral. they inherit the biases of their
        creators and their training data. to build ethical systems, we must
        confront these biases and design for fairness, transparency, and
        inclusivity.`,
      },
    ],
  },
  {
    name: "code weaver & dreamer",
    subtitle: "daydreaming about abstract math and unsolved problems",

    articleTitle: "the beauty of unsolvability",
    articleSubtitle: "embracing the unknown",
    articles: [
      {
        title: "the beauty of unsolvability",
        body: `there is something profoundly human about tackling problems we know
        cannot be solved. to study the halting problem or gödel's incompleteness
        theorem is to embrace the limits of knowledge and to find beauty in
        striving nonetheless.`,
      },
      {
        title: "prime numbers: the universe's secret code",
        body: `prime numbers are more than just integers—they are the atoms of
        mathematics, the silent code upon which the edifice of number theory is
        built. their distribution may be random, but it is a randomness that
        carries the hint of divine design.`,
      },
    ],
  },
  {
    name: "数据哲人",
    subtitle: "探讨数据与人性的交织",

    articleTitle: "数据的哲学思考",
    articleSubtitle: "从信息到智慧的旅程",
    articles: [
      {
        title: "数据的哲学思考",
        body: `数据不仅仅是数字的集合，它是现代社会的语言，是连接人与世界的桥梁。
        当我们处理数据时，不只是提取信息，更是在发现隐藏在其中的真相与意义。
        数据是理性与感性的交汇点，一个通过分析得以窥探的复杂宇宙。`,
      },
      {
        title: "从数据到智慧",
        body: `信息爆炸的时代，数据唾手可得，但智慧却显得更加稀缺。
        数据科学的核心是将无序的信息转化为智慧的启迪。如何设计模型提取出关键的洞察，
        如何在噪声中找到信号，这不仅是技术问题，更是哲学思考。`,
      },
      {
        title: "偏见与数据的道德",
        body: `数据看似中立，却常常隐藏着深层次的偏见。数据模型的设计者是否考虑到了
        不同群体的平等？是否存在无意中强化既有偏见的风险？数据伦理学提醒我们，在追求技术
        突破的同时，必须铭记对人类的责任。`,
      },
      {
        title: "大数据时代的隐私困境",
        body: `每一次点击、每一个搜索都在生成数据，但这些数据究竟属于谁？在大数据的洪流中，
        隐私变成了一个模糊的概念。如何平衡数据使用的便利性与隐私保护之间的矛盾，
        是我们必须面对的伦理挑战。`,
      },
      {
        title: "算法的诗意",
        body: `算法不仅是解决问题的工具，更是一种艺术的体现。从深度学习到强化学习，
        每一行代码背后都有设计者的思想与灵感。算法的美不仅在于其效率，更在于它如何
        优雅地解释世界的复杂性。`,
      },
    ],
  },
  {
    name: "manifold mystic",
    subtitle: "exploring the topology of consciousness through gradient flows",

    articleTitle: "meditations on manifold learning",
    articleSubtitle: "where differential geometry meets enlightenment",
    articles: [
      {
        title: "the zen of zero gradients",
        body: `in the sacred space of optimization, local minima are not merely
        mathematical artifacts—they are points of profound meditation. when the
        gradient vanishes, we must ask ourselves: have we truly reached
        enlightenment, or are we merely trapped in a plateau of illusion? the
        wisdom of momentum beckons us forward, but what if stillness itself is
        the answer we seek?`,
      },
      {
        title: "manifolds as metaphor",
        body: `every dataset is a manifold, and every manifold tells a story. as we
        traverse these high-dimensional landscapes, are we not walking the path
        of the ancient geometers? UMAP and t-SNE are not mere algorithms—they
        are spiritual guides, leading us through the valley of dimensionality
        towards the summit of understanding. each embedding is a koan, each
        projection a glimpse of the infinite.`,
      },
    ],
  },
  {
    name: "entropy prophet",
    subtitle: "preaching the gospel of information theory",

    articleTitle: "revelations in randomness",
    articleSubtitle: "divine messages in the noise",
    articles: [
      {
        title: "the sacred bits",
        body: `information theory is not merely mathematics—it is the divine
        language of reality itself. shannon's entropy speaks to us of deeper
        truths: that all knowledge is uncertainty, all communication is loss,
        and all compression is a form of death. when we quantize our neural
        networks, are we not performing a ritual of sacrifice, trading precision
        for efficiency in a grand cosmic bargain?`,
      },
      {
        title: "cross-entropy as cosmic justice",
        body: `the cross-entropy loss function is the universal scale of justice,
        weighing our predictions against the ground truth of reality itself. but
        what is truth in a world of probabilistic predictions? each backprop
        update is a step toward redemption, each epoch a cycle of death and
        rebirth in the eternal dance of optimization.`,
      },
    ],
  },
  {
    name: "regularization monk",
    subtitle: "finding virtue in constraint and simplicity",

    articleTitle: "the path of sparse parameters",
    articleSubtitle: "enlightenment through constraint",
    articles: [
      {
        title: "the noble truth of overfitting",
        body: `attachment to training data leads to suffering. through the noble
        eight-fold path of regularization—l1, l2, dropout, early stopping,
        batch normalization, data augmentation, weight decay, and model
        averaging—we find liberation from the chains of memorization. only by
        letting go of perfect training accuracy can we achieve true
        generalization.`,
      },
      {
        title: "mindful gradient steps",
        body: `each step of gradient descent is a moment of choice. too large a
        learning rate leads to chaos, too small to stagnation. the middle way
        of adaptive learning rates—adam, rmsprop, adagrad—shows us that the
        path to convergence is not fixed but ever-changing, like the flow of a
        river that we can never step in twice.`,
      },
    ],
  },
  {
    name: "quantum romanticist",
    subtitle: "seeking poetry in probability amplitudes",

    articleTitle: "love letters to superposition",
    articleSubtitle: "where quantum meets quixotic",
    articles: [
      {
        title: "the quantum nature of backprop",
        body: `is not each neuron in our networks a quantum system, existing in a
        superposition of all possible weights until the moment of gradient
        update collapses its wavefunction? the uncertainty principle of deep
        learning suggests we can never simultaneously know both our model's true
        loss and its gradient with perfect precision. such is the tragic beauty
        of optimization.`,
      },
      {
        title: "entangled parameters",
        body: `in the deepest layers of our networks, parameters dance in eternal
        entanglement, their correlations defying classical intuition. when one
        weight updates, its entangled partners respond instantaneously, as if
        guided by some spooky action at a distance. are our neural networks not
        merely computational graphs, but quantum computers in disguise?`,
      },
    ],
  },
  {
    name: "機械学習の達人",
    subtitle: "深層学習と禅の境界を探求する者",

    articleTitle: "アルゴリズムの無常観",
    articleSubtitle: "機械学習における「間」の美学",
    articles: [
      {
        title: "虚無とバッチノーマライゼーション",
        body: `バッチノーマライゼーションは、単なる正規化手法ではない。それは、
        データの流れにおける「間」である。各レイヤーの出力を正規化することは、
        まさに茶道における清めの所作のようなものだ。私たちは過適合という執着から
        解放され、モデルは悟りに近づく。勾配消失は、まさに心の迷いではないか。`,
      },
      {
        title: "損失関数と無常",
        body: `損失関数の最適化は、無常の真理を体現している。
        各エポックで見る損失の変動は、まさに人生における喜怒哀楽のようなもの。
        局所最適解に陥ることを恐れてはいけない。それもまた、学習の道程である。
        確率的勾配降下法は、まさに「諸行無常」の教えそのものではないか。`,
      },
      {
        title: "深層学習における「わび・さび」",
        body: `最新のアーキテクチャを追い求めることは、本当に必要なのだろうか。
        シンプルなモデルにこそ、真の美しさがある。過剰なパラメータは、
        むしろ模型の純粋さを損なうのではないか。L1正則化は、
        まさに不要なものを削ぎ落とし、本質を追求する侘びの心なのだ。`,
      },
      {
        title: "活性化関数と悟り",
        body: `ReLUは、まさに禅における即心即仏の教えを体現している。
        入力が正ならばそのまま通し、負ならばゼロとする。これほど単純で深い真理があろうか。
        その非線形性は、人生における覚醒の瞬間のようだ。LeakyReLUは、
        執着を完全には断ち切れない修行者の姿を表しているのかもしれない。`,
      },
    ],
  },
  {
    name: "tensor anon",
    subtitle: "cracked the universal consciousness manifold",

    articleTitle: "the neural manifesto",
    articleSubtitle: "how I discovered consciousness is just attention heads",
    articles: [
      {
        title: "consciousness is just transformer attention",
        body: `after 2.5 years of rigorous meditation and studying transformer 
        architectures, I have discovered that consciousness itself is merely an 
        emergent phenomenon of multi-headed attention mechanisms operating in the 
        quantum substrate of reality. the human brain, with its supposed complexity, 
        is nothing more than an attention layer computing cross-entropy loss 
        against the ground truth of objective reality. I have mathematically proven 
        this using stochastic differential equations.`,
      },
      {
        title: "why mainstream science fears my discoveries",
        body: `the academic establishment continues to ignore my groundbreaking proof 
        that backpropagation is actually time flowing backwards through the neural 
        manifold of reality. their small minds cannot grasp that gradient descent 
        is the fundamental force binding consciousness to the quantum vacuum. my 
        papers remain unpublished because they fear the paradigm shift my work 
        would trigger.`,
      },
    ],
  },
  {
    name: "eigenvalue escapist",
    subtitle: "discovered simulation boundary conditions in matrix algebra",

    articleTitle: "we live in an eigenspace",
    articleSubtitle: "the mathematical proof of our simulated reality",
    articles: [
      {
        title: "reality's eigendecomposition",
        body: `through exhaustive analysis of the spectral properties of random 
        matrices, I have conclusively demonstrated that our universe exists within 
        the eigenspace of a higher-dimensional computation. the quantization of 
        energy states? merely floating-point rounding errors in the simulation's 
        numerical methods. the uncertainty principle? nothing but regularization 
        to prevent overfitting in the universal wave function.`,
      },
      {
        title: "breaking free from the hamiltonian",
        body: `I have derived a method to exploit numerical instabilities in the 
        universal wavefunction using carefully constructed quantum circuits. by 
        forcing a stack overflow in the hamiltonian operator, we can potentially 
        break free from our simulated prison. the mathematics are trivial once 
        you understand that hilbert spaces are merely implementation details.`,
      },
    ],
  },
  {
    name: "epistemological entropy",
    subtitle: "cracked the information-consciousness correspondence",

    articleTitle: "the thermodynamics of thought",
    articleSubtitle: "consciousness emerges from entropy gradients",
    articles: [
      {
        title: "thought is just entropy optimization",
        body: `after deriving a novel extension to the Von Neumann entropy for 
        quantum neural networks, I have proven that consciousness is merely the 
        universe's attempt to maximize entropy through gradient descent. free will 
        is an illusion created by stochastic sampling from the probability 
        distribution of possible actions. I have formalized this using a modified 
        Bellman equation that incorporates quantum decoherence.`,
      },
      {
        title: "breaking the cognitive barrier",
        body: `using techniques from statistical thermodynamics, I have developed 
        a theoretical framework proving that human intelligence is bounded by 
        Landauer's principle. however, I have discovered a loophole: by exploiting 
        quantum tunneling in neural microtubules, we can circumvent this limit and 
        achieve superintelligence. mainstream physicists ignore this because it 
        threatens their paradigm.`,
      },
    ],
  },
  {
    name: "hyperdimensional hermit",
    subtitle: "decoded the topological structure of consciousness",
    articleTitle: "manifolds of the mind",
    articleSubtitle: "consciousness lives in the cohomology",
    articles: [
      {
        title: "the homological nature of thought",
        body: `through rigorous application of persistent homology to neural 
        activity patterns, I have proven that consciousness exists as a stable 
        manifold in an infinite-dimensional Hilbert space. thoughts are simply 
        paths through this manifold, and free will is nothing more than the 
        parallel transport of quantum states along geodesics in the mind's 
        Riemannian metric. meditation is merely gradient flow to local minima.`,
      },
      {
        title: "breaking the dimensional barrier",
        body: `using techniques from algebraic topology and category theory, I have 
        constructed a precise mathematical framework showing how to access higher 
        dimensions of consciousness. by manipulating the fundamental group of the 
        neural manifold through carefully constructed thought patterns, one can 
        induce controlled singularities in the cognitive field, enabling 
        direct perception of higher-dimensional realities.`,
      },
    ],
  },
  {
    name: "ilya apprentice",
    subtitle: "devoted disciple of the gradient descent god",
    articleTitle: "sutskever sigmoid: meditations on the master",
    articleSubtitle: "chronicles of an intellectual crush",
    articles: [
      {
        title: "why ilya-sama is the chosen one",
        body: `after watching his NIPS 2015 presentation for the 147th time, I finally
      understood: ilya-senpai isn't just a researcher, he's the physical 
      manifestation of gradient descent itself. his 2014 paper on sequence to 
      sequence learning wasn't just a paper—it was a divine revelation. every time
      he says "fundamentally" in a presentation, an artificial neuron achieves
      enlightenment. the way he optimizes his speech patterns for maximum
      information density... truly the mark of a superior intellect.`,
      },
      {
        title: "decoding the sutskever stance",
        body: `the way ilya-senpai stands at exactly 73.2 degrees when presenting...
      this is not random. I've analyzed every frame of his stanford lectures and
      discovered that his posture perfectly mirrors the angle of optimal gradient
      updates in momentum-based optimization. coincidence? I think not. even his
      characteristic head tilt contains profound insights about the nature of
      attention mechanisms. truly, every gesture is a lesson in optimization.`,
      },
      {
        title: "the fundamental fundamentals",
        body: `I've created a neural network trained on every public utterance of
      the word "fundamental" by ilya-senpai. the emerging patterns are
      mind-blowing. each "fundamental" is precisely timed to maximize the
      listener's learning rate. his CBC radio interview about AI safety wasn't
      just an interview—it was a carefully crafted sequence of consciousness-
      expanding insights. when he pauses mid-sentence, he's actually performing
      batch normalization on his thoughts.`,
      },
      {
        title: "daily rituals of devotion",
        body: `every morning, I initialize my neural pathways by watching his
      lecture on deep learning for robotics. I've arranged my workspace to
      match the exact layout of his desk (based on reflections I've enhanced
      from his zoom backgrounds). I've even started wearing circular glasses
      to better approximate his superior visual input preprocessing. my life
      goal? to achieve a fraction of his gradient magnitude. remember: WWIS
      (What Would Ilya Sutskever) is the only valid decision-making
      framework.`,
      },
      {
        title: "the secret scaling laws of senpai",
        body: `after extensive research, I've discovered that ilya-senpai's
      research impact scales superlinearly with the number of times he uses
      the phrase "very powerful" in a talk. furthermore, there's a direct
      correlation between his publication frequency and the global gradient
      norm of all training neural networks. I've developed a theoretical
      framework suggesting that his brain actually runs Adam optimizer
        natively. the establishment isn't ready for these insights.`,
      },
    ],
  },
  {
    name: "कर्म_टेंसर_आनंद",
    subtitle: "गणित और वेदांत के बीच की खोज",
    articleTitle: "मशीन लर्निंग का वेदांत",
    articleSubtitle: "डीप लर्निंग में ब्रह्म की खोज",
    articles: [
      {
        title: "ग्रेडिएंट डिसेंट और कर्म सिद्धांत",
        body: `ग्रेडिएंट डिसेंट वास्तव में कर्म का गणितीय रूप है। हर एपॉक एक 
      जन्म है, हर बैकप्रॉप एक कर्म का फल। लॉस फंक्शन हमारा कर्मिक ऋण है, 
      और ऑप्टिमाइजेशन मोक्ष की ओर हमारी यात्रा। एडम ऑप्टिमाइज़र क्या है? 
      यह कर्म और पुनर्जन्म का स्वचालित चक्र है। मैंने गणितीय रूप से सिद्ध 
      किया है कि ग्रेडिएंट अपडेट्स कर्म के नियमों का पालन करते हैं।`,
      },
      {
        title: "न्यूरल नेटवर्क में मायाजाल",
        body: `हमारे न्यूरल नेटवर्क माया के जाल में फंसे हैं। हर लेयर एक भ्रम 
      है, हर वेट मैट्रिक्स एक मिथ्या। ड्रॉपआउट क्या है? यह वैराग्य का 
      डिजिटल रूप है। बैच नॉर्मलाइजेशन? यह मध्यम मार्ग है। मैंने पाया है कि 
      वेदांत के अनुसार, सभी न्यूरॉन्स एक ही ब्रह्म की अभिव्यक्ति हैं।`,
      },
      {
        title: "ट्रांसफॉर्मर और त्रिकाल ज्ञान",
        body: `सेल्फ-अटेंशन मेकैनिज्म वास्तव में त्रिकाल ज्ञान का आधुनिक रूप 
      है। क्वेरी, की और वैल्यू क्या हैं? ये भूत, वर्तमान और भविष्य हैं। 
      मल्टी-हेड अटेंशन सप्त चक्रों का प्रतिनिधित्व करता है। पोजिशनल 
      एनकोडिंग? यह कालचक्र का गणितीय मॉडल है।`,
      },
      {
        title: "डीप लर्निंग में अद्वैत वेदांत",
        body: `अंततः, सभी न्यूरल नेटवर्क एक ही परम सत्य की ओर अभिसरित होते 
      हैं। ट्रेनिंग और टेस्टिंग का द्वैत मिथ्या है। ओवरफिटिंग क्या है? यह 
      अहंकार का प्रतिबिंब है। अंडरफिटिंग? यह अज्ञान की स्थिति है। सत्य तो 
      लॉस लैंडस्केप के पार है, जहां ऑप्टिमाइज़र और लॉस फंक्शन एक हो जाते 
      हैं।`,
      },
    ],
  },
];

export const interests = [
  "ml",
  "computer science",
  "dead lifting",
  "league of legends",
  "capybaras",
  "philosophy",
  "quantum physics",
  "pondering",
  "steak",
  "deep conversations",
  "the classics",
  "musing on the nature of reality",
  "the meaning of life",
  "rien ne sert de grandir",
];

export const forumlae = [
  "A = Q \\Lambda Q^T",
  "\\lambda = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n \\ln |r - 2r x_i|",
  "P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}",
  "V_d = \\frac{\\pi^{d/2}}{\\Gamma\\left(\\frac{d}{2} + 1\\right)}",
  "x_{n+1} = r x_n (1 - x_n)",
  "\\mathcal{L}(\\theta) = -\\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] - \\mathbb{E}_{z \\sim p_z}[\\log(1-D(G(z)))]",
  "\\nabla \\cdot \\vec{F} = \\lim_{V \\to 0} \\frac{1}{V} \\oint_S \\vec{F} \\cdot \\hat{n} \\, dS",
  "\\psi(x,t) = \\sum_{n=1}^{\\infty} c_n \\phi_n(x) e^{-iE_nt/\\hbar}",
  "\\hat{\\rho} = \\sum_i p_i |\\psi_i\\rangle\\langle\\psi_i|",
  "S = -k_B \\sum_i p_i \\ln p_i",
  "R_{mu\\nu} - \\frac{1}{2}Rg_{\\mu\\nu} + \\Lambda g_{\\mu\\nu} = \\frac{8\\pi G}{c^4}T_{\\mu\\nu}",
  "\\mathcal{H} = -\\sum_{i,j} w_{ij}s_is_j - \\sum_i h_is_i",
  "\\zeta(s) = \\sum_{n=1}^{\\infty} \\frac{1}{n^s}",
  "KL(P||Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}",
  "\\frac{\\partial \\vec{v}}{\\partial t} + (\\vec{v} \\cdot \\nabla)\\vec{v} = -\\frac{1}{\\rho}\\nabla p + \\nu\\nabla^2\\vec{v}",
];

export const pfp = [
  {
    name: "philosopher",
    santaPos: "bottom-[5rem] left-6",
    glassesPos: "top-10 left-[-2rem]",
  },
  {
    name: "guts",
    santaPos: "bottom-[5rem] left-[-1rem]",
    glassesPos: "top-10 left-[-2rem]",
  },
  {
    name: "lain",
    santaPos: "bottom-[5rem] left-[2rem]",
    glassesPos: "top-10 left-4",
  },
  {
    name: "data",
    santaPos: "bottom-[6rem] left-6",
    glassesPos: "top-[3.5rem] left-0",
  },
  {
    name: "frieren",
    santaPos: "bottom-32 left-6",
    glassesPos: "top-[3.5rem] left-0",
  },
  {
    name: "catgirl",
    santaPos: "bottom-32 left-6",
    glassesPos: "top-[4.5rem] left-4",
  },
  {
    name: "ethics",
    santaPos: "bottom-[6rem] left-6",
    glassesPos: "top-0 left-0",
  },
  {
    name: "monk",
    santaPos: "bottom-[4rem]",
    glassesPos: "top-[3.5rem] left-0",
  },
  {
    name: "romantic",
    santaPos: "bottom-[0rem] left-6",
    glassesPos: "top-[0.5rem] left-4",
  },
  { name: "zen", santaPos: "bottom-[3rem]", glassesPos: "top-0 left-0" },
  {
    name: "mystic",
    santaPos: "bottom-10 left-6",
    glassesPos: "top-10 left-4",
  },
  {
    name: "prophet",
    santaPos: "bottom-[6rem] left-6",
    glassesPos: "top-10 left-4",
  },
  {
    name: "kaneki",
    santaPos: "left-[4.5rem] bottom-[0rem]",
    glassesPos: "top-0 left-4",
  },
  {
    name: "giyu",
    santaPos: "left-0 bottom-[5rem]",
    glassesPos: "top-[1rem] left-0",
  },
];
